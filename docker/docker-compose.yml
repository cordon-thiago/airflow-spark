version: "3.7"

x-dag-path: &dag_path ../code/dags:/usr/local/airflow/dags #DAG folder
x-spark_path: &spark_path ../code/spark:/usr/local/spark/app #Spark Scripts (Must be the same path in airflow and Spark Cluster)
x-resources_path: &resources_path ../resources:/usr/local/spark/resources #Resources folder (Must be the same path in airflow and Spark Cluster)

x-spark_hadoop_version: &spark_hadoop_version
    - SPARK_VERSION=3.2.0
    - HADOOP_VERSION=3.2

x-spark_image: &spark_image bitnami/spark:3.2.0

x-spark-worker-env: &spark-worker-env
    - SPARK_MODE=worker
    - SPARK_MASTER_URL=spark://spark:7077
    - SPARK_WORKER_MEMORY=1G
    - SPARK_WORKER_CORES=1
    - SPARK_RPC_AUTHENTICATION_ENABLED=no
    - SPARK_RPC_ENCRYPTION_ENABLED=no
    - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
    - SPARK_SSL_ENABLED=no


services:  
    # postgres timescale
    postgres:
        #image: postgres:13
        build:
            context: ..
            dockerfile: docker/docker-postgres-timescale/Dockerfile
        networks:
            - default_net
        environment:
            - POSTGRES_USER=airflow
            - POSTGRES_PASSWORD=airflow
            - POSTGRES_DB=airflow
        healthcheck:
            test: [ "CMD", "pg_isready", "-U", "airflow" ]
            interval: 5s
            retries: 5
        ports:
            - "5432:5432"

    # airflow LocalExecutor
    airflow-webserver:
        build:
            context: ..
            dockerfile: docker/docker-airflow/Dockerfile
            args: *spark_hadoop_version
        restart: always
        networks:
            - default_net
        depends_on:
            - postgres
        environment:
            - LOAD_EX=n
            - EXECUTOR=Local
            - FERNET_KEY=pe1SS1cu1l1sMVlz-uT3Bg5v3z1rQJp0RrJW6XLO0XM=
            - AIRFLOW__CORE__FERNET_KEY=pe1SS1cu1l1sMVlz-uT3Bg5v3z1rQJp0RrJW6XLO0XM=
            - AIRFLOW_CONN_POSTGRES_TEST=postgres://test:postgres@postgres:5432/test
            - AIRFLOW_CONN_SPARK_DEFAULT=spark://spark:7077
        volumes:
            - *dag_path
            - *spark_path
            - *resources_path
        ports:
            - "8282:8282"
        command: webserver
        healthcheck:
            test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]
            interval: 30s
            timeout: 30s
            retries: 3

    # Spark with 3 workers
    spark:
        image: *spark_image
        user: root # Run container as root container: https://docs.bitnami.com/tutorials/work-with-non-root-containers/
        hostname: spark
        networks:
            - default_net
        environment:
            - SPARK_MODE=master
            - SPARK_RPC_AUTHENTICATION_ENABLED=no
            - SPARK_RPC_ENCRYPTION_ENABLED=no
            - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
            - SPARK_SSL_ENABLED=no
        volumes:
            - *spark_path
            - *resources_path
        ports:
            - "8181:8080"
            - "7077:7077"

    spark-worker-1:
        image: *spark_image
        user: root
        networks:
            - default_net
        environment: *spark-worker-env
        volumes:
            - *spark_path
            - *resources_path

    spark-worker-2:
        image: *spark_image
        user: root
        networks:
            - default_net
        environment: *spark-worker-env
        volumes:
            - *spark_path
            - *resources_path

    spark-worker-3:
        image: *spark_image
        user: root
        networks:
            - default_net
        environment: *spark-worker-env
        volumes:
            - *spark_path
            - *resources_path

    rstudio:
        #image: rocker/rstudio:4.0.0
        deploy:
            # does this even make sense?
            resources:
                limits:
                    memory: 1024MB
                reservations:
                    memory: 600MB
        networks:
            - default_net
        build:
            context: ..
            dockerfile: docker/docker-rstudio/Dockerfile
            args: *spark_hadoop_version
        restart: on-failure
        volumes:
            - ../code/app_airflow_spark:/home/rstudio/app_airflow_spark
            - ../code/notebooks:/home/rstudio/notebooks
            - ../code/dags:/home/rstudio/dags
            - ../code/spark:/home/rstudio/spark
            - *resources_path
        environment:
            - PASSWORD=airflow
        ports:
            - "8787:8787"
            - "3737:3737"
networks:
    default_net: