version: "3.7"

x-dag-path: &dag_path ./code/dags:/usr/local/airflow/dags #DAG folder
x-spark_path: &spark_path ./code/spark:/usr/local/spark/app #Spark Scripts (Must be the same path in airflow and Spark Cluster)
x-resources_path: &resources_path ./resources:/usr/local/spark/resources #Resources folder (Must be the same path in airflow and Spark Cluster)
x-data_storage_path: &data_storage_path ./local_data:/usr/local/remote_data
x-modules_path: &modules_path ./code/modules:/usr/local/modules
x-sql_path: &sql_path ./code/sql:/usr/local/sql

x-spark_hadoop_version: &spark_hadoop_version
    - SPARK_VERSION=3.2.0
    - HADOOP_VERSION=3.2

x-spark_image: &spark_image bitnami/spark:3.2.0

x-spark-worker-env: &spark-worker-env
    - SPARK_MODE=worker
    - SPARK_MASTER_URL=spark://spark:7077
    - SPARK_WORKER_MEMORY=1G
    - SPARK_WORKER_CORES=1
    - SPARK_RPC_AUTHENTICATION_ENABLED=no
    - SPARK_RPC_ENCRYPTION_ENABLED=no
    - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
    - SPARK_SSL_ENABLED=no


services:  
    # postgres timescale
    postgres:
        build:
            dockerfile: docker/docker-postgres-timescale/Dockerfile
        restart: on-failure
        networks:
            - default_net
        environment:
            - POSTGRES_USER=airflow
            - POSTGRES_PASSWORD=airflow
            - POSTGRES_DB=airflow
        healthcheck:
            test: [ "CMD", "pg_isready", "-U", "airflow" ]
            interval: 5s
            retries: 5
        ports:
            - "5432:5432"

    # airflow LocalExecutor
    airflow-webserver:
        build:
            dockerfile: docker/docker-airflow/Dockerfile
            args: *spark_hadoop_version
        restart: on-failure
        networks:
            - default_net
        depends_on:
            - postgres
        environment:
            - LOAD_EX=n
            - EXECUTOR=Local
            - AIRFLOW__CORE__LOAD_EXAMPLES=False
            - FERNET_KEY=pe1SS1cu1l1sMVlz-uT3Bg5v3z1rQJp0RrJW6XLO0XM=
            - AIRFLOW__CORE__FERNET_KEY=pe1SS1cu1l1sMVlz-uT3Bg5v3z1rQJp0RrJW6XLO0XM=
            - AIRFLOW_CONN_POSTGRES_TEST=postgres://test:postgres@postgres:5432/test
            - AIRFLOW_CONN_SPARK_DEFAULT=spark://spark:7077
        volumes:
            - *dag_path
            - *spark_path
            - *resources_path
            - *data_storage_path
            - *modules_path
            - *sql_path
        ports:
            - "8282:8282"
        command: webserver
        healthcheck:
            test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]
            interval: 30s
            timeout: 30s
            retries: 3

    rstudio:
        networks:
            - default_net
        build:
            dockerfile: docker/docker-rstudio/Dockerfile
            args: *spark_hadoop_version
        restart: on-failure
        volumes:
            - ./code:/home/rstudio
            - *resources_path
            - *data_storage_path
            - *modules_path
            - *sql_path
        environment:
            - PASSWORD=airflow
        ports:
            - "8888:8787"
            - "3737:3737"

    dask-scheduler:
        image: daskdev/dask:2021.9.1-py3.8
        #hostname: scheduler
        networks:
            - default_net
        volumes:
            - *spark_path
            - *resources_path
            - *data_storage_path
            - *modules_path
        ports:
            - "8786:8786"
            - "8787:8787"
        environment:
            - EXTRA_PIP_PACKAGES=psycopg2-binary==2.9.3 SQLAlchemy==1.3.24 scikit-learn==1.0.2
        command: [ "dask-scheduler" ]

    dask-worker:
        image: daskdev/dask:2021.9.1-py3.8
        command: [ "dask-worker", "tcp://dask-scheduler:8786" ]
        volumes:
            - *spark_path
            - *resources_path
            - *data_storage_path
            - *modules_path
        environment:
            - EXTRA_PIP_PACKAGES=psycopg2-binary==2.9.3 SQLAlchemy==1.3.24 scikit-learn==1.0.2
        networks:
            - default_net
        deploy:
            replicas: 2

networks:
    default_net: